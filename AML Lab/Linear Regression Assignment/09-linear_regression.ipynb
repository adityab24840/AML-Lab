{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "from plots import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\Large y = f(x) = wx + b + \\epsilon\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Prepare data for Linear Regression\n",
    "\n",
    "##### 1.1 Generate synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_b = 1\n",
    "true_w = 2\n",
    "N = 100\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create an array of the given shape and populate it with random samples \n",
    "# from a uniform distribution over [0, 1)\n",
    "X = np.random.rand(N, 1)\n",
    "epsilon = (.1 * np.random.randn(N, 1))\n",
    "\n",
    "y = true_b + true_w * X + epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame({ \"x\": X.tolist(), \"y\": y.tolist()})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X, y)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.2 Train Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffles the indices\n",
    "idx = np.arange(N)\n",
    "np.random.shuffle(idx)\n",
    "\n",
    "# Uses first 80 random indices for train\n",
    "train_idx = idx[:int(N*.8)]\n",
    "\n",
    "# Uses the remaining indices for validation\n",
    "val_idx = idx[int(N*.8):]\n",
    "\n",
    "# Generates train and validation sets\n",
    "X_train, y_train = X[train_idx], y[train_idx]\n",
    "X_val, y_val = X[val_idx], y[val_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_train_test(X_train, y_train, X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Random intialization and initial fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 0 - Initializes parameters \"b\" and \"w\" randomly\n",
    "np.random.seed(42)\n",
    "b = np.random.randn(1)\n",
    "w = np.random.randn(1)\n",
    "\n",
    "print(b, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = b + w * X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_fit(X_train, y_train, b, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\Large \\text{error}^{(i)} = \\hat{y}^{(i)} - y^{(i)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_fit_error(X_train, y_train, b, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Objective/Lost/Cost Function\n",
    "\n",
    "$$\n",
    "\\Large\n",
    "\\begin{aligned}\n",
    "\\text{MSE} &= \\frac{1}{n} \\sum_{i=1}^n{\\text{error}^{(i)}}^2\n",
    "\\\\\n",
    "&= \\frac{1}{n} \\sum_{i=1}^n{(\\hat{y}^{(i)} - y^{(i)})}^2\n",
    "\\\\\n",
    "&= \\frac{1}{n} \\sum_{i=1}^n{(b + w x^{(i)} - y^{(i)})}^2\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2 - Computing the loss\n",
    "# We are using ALL data points, so this is BATCH gradient\n",
    "# descent. How wrong is our model? That's the error!\n",
    "error = (yhat - y_train)\n",
    "\n",
    "# It is a regression, so it computes mean squared error (MSE)\n",
    "loss = (error ** 2).mean()\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.1 Plotting the loss surface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reminder:\n",
    "# true_b = 1\n",
    "# true_w = 2\n",
    "\n",
    "# we have to split the ranges in 100 evenly spaced intervals each\n",
    "# b from -2 to 4, w from -1 to 5\n",
    "b_range = np.linspace(true_b - 3, true_b + 3, 101)\n",
    "w_range = np.linspace(true_w - 3, true_w + 3, 101)\n",
    "\n",
    "# values for all combinations\n",
    "bs, ws = np.meshgrid(b_range, w_range)\n",
    "bs.shape, ws.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_x = X_train[0]\n",
    "sample_yhat = bs + ws * sample_x\n",
    "sample_yhat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_predictions = np.apply_along_axis(\n",
    "    func1d=lambda x: bs + ws * x, \n",
    "    axis=1, \n",
    "    arr=X_train\n",
    ")\n",
    "all_predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_labels = y_train.reshape(-1, 1, 1)\n",
    "all_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_errors = (all_predictions - all_labels)\n",
    "all_errors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_losses = (all_errors ** 2).mean(axis=0)\n",
    "all_losses.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "range_plot(X_train, y_train, b, w, bs, ws, all_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.2 Cross section analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_section_fixed_b(X_train, y_train, b, w, bs, ws, all_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_section_fixed_w(X_train, y_train, b, w, bs, ws, all_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Gradient Descent Intuition\n",
    "\n",
    "$$\n",
    "\\Large\n",
    "\\mathcal{J}(w, b) = MSE = \\frac{1}{n} \\sum_{i=1}^n{(b + w x^{(i)} - y^{(i)})}^2 \\\\\n",
    "\\hspace{4 mm} \\\\\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial{\\text{MSE}}}{\\partial{b}} = \\frac{\\partial{\\text{MSE}}}{\\partial{\\hat{y}^{(i)}}} \\frac{\\partial{\\hat{y}^{(i)}}}{\\partial{b}} &= \\frac{1}{n} \\sum_{i=1}^n{2(b + w x^{(i)} - y^{(i)})} \n",
    "&= 2 \\frac{1}{n} \\sum_{i=1}^n{(\\hat{y}^{(i)} - y^{(i)})}\n",
    "\\\\\n",
    "\\frac{\\partial{\\text{MSE}}}{\\partial{w}} = \\frac{\\partial{\\text{MSE}}}{\\partial{\\hat{y}^{(i)}}} \\frac{\\partial{\\hat{y}^{(i)}}}{\\partial{w}} &= \\frac{1}{n} \\sum_{i=1}^n{2(b + w x^{(i)} - y^{(i)}) x^{(i)}} \n",
    "&= 2 \\frac{1}{n} \\sum_{i=1}^n{x^{(i)} (\\hat{y}^{(i)} - y^{(i)})}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute gradients for both \"b\" and \"w\"\n",
    "b_grad = 2 * error.mean()\n",
    "w_grad = 2 * (X_train * error).mean()\n",
    "print(b_grad, w_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.1 Visualize Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_gradient(b, w, bs, ws, all_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotated_delta_sensitivity(b, w, bs, ws, all_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Gradient Descent (aka Backpropagation)\n",
    "\n",
    "$$\n",
    "\\Large\n",
    "\\begin{aligned}\n",
    "b &= b - \\eta \\frac{\\partial{\\text{MSE}}}{\\partial{b}}\n",
    "\\\\\n",
    "w &= w - \\eta \\frac{\\partial{\\text{MSE}}}{\\partial{w}}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets learning rate - this is \"eta\" ~ the \"n\" like Greek letter\n",
    "lr = 0.1\n",
    "print(b, w)\n",
    "\n",
    "# error = wx + b - yi\n",
    "b_grad = 2 * error.mean() \n",
    "w_grad = 2 * (X_train * error).mean()\n",
    "print(b_grad, w_grad)\n",
    "\n",
    "# Update parameters using gradients and learning rate\n",
    "b = b - lr * b_grad\n",
    "w = w - lr * w_grad\n",
    "\n",
    "print(b, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_line_after_single_descent(X_train, y_train, b, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.1 Learning rate related issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_grad_b = -2.90\n",
    "manual_grad_w = -1.79\n",
    "\n",
    "np.random.seed(42)\n",
    "b_initial = np.random.randn(1)\n",
    "w_initial = np.random.randn(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Low Learning_rate**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate - greek letter \"eta\" that looks like an \"n\"\n",
    "lr = .2\n",
    "\n",
    "learning_rate_plot(b_initial, w_initial, bs, ws, all_losses, manual_grad_b, manual_grad_w, lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**High learning rate**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate - greek letter \"eta\" that looks like an \"n\"\n",
    "lr = .8\n",
    "\n",
    "learning_rate_plot(b_initial, w_initial, bs, ws, all_losses, manual_grad_b, manual_grad_w, lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Very high learning rate**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate - greek letter \"eta\" that looks like an \"n\"\n",
    "lr = 1.1\n",
    "\n",
    "learning_rate_plot(b_initial, w_initial, bs, ws, all_losses, manual_grad_b, manual_grad_w, lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Putting it all together\n",
    "\n",
    "##### A. Actual analytical function on the population\n",
    "$$\n",
    "\\Large\n",
    "y^{(i)} = f(x^{(i)})\n",
    "$$\n",
    "\n",
    "##### B. On a sample, in reality\n",
    "$$\n",
    "\\Large\n",
    "y^{(i)} = f(x^{(i)}) + \\epsilon\n",
    "$$\n",
    "\n",
    "where $ \\epsilon $ is noise.\n",
    "\n",
    "\n",
    "##### C. Hypothesis function\n",
    "$$\n",
    "\\Large\n",
    "\\hat{y}^{(i)} = \\mathcal{h}(x^{(i)}; w, b) = wx^{(i)} + b\n",
    "$$\n",
    "\n",
    "##### D. Cost function\n",
    "$$\n",
    "\\Large\n",
    "\\begin{aligned}\n",
    "\\mathcal{J}(w, b) = MSE & = \\frac{1}{2n} \\sum_{i=1}^n{(b + w x^{(i)} - y^{(i)})}^2 \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "| What | Details | \n",
    "| --- | --- |\n",
    "| Parameters | w, b | \n",
    "| Objective | $ \\arg \\min_{w,b} \\mathcal{J}(w, b) $ |\n",
    "\n",
    "\n",
    "##### E. Analytical gradient\n",
    "$$\n",
    "\\Large\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial{\\text{MSE}}}{\\partial{b}} & = \\frac{1}{n} \\sum_{i=1}^n{(\\hat{y}^{(i)} - y^{(i)})} \\\\\n",
    "\\frac{\\partial{\\text{MSE}}}{\\partial{w}} & = \\frac{1}{n} \\sum_{i=1}^n{x^{(i)} (\\hat{y}^{(i)} - y^{(i)})}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "##### F. Iterative numerical gradient descent\n",
    "\n",
    "$$\n",
    "\\Large\n",
    "\\begin{aligned}\n",
    "b &= b - \\eta \\frac{\\partial{\\text{MSE}}}{\\partial{b}}\n",
    "\\\\\n",
    "w &= w - \\eta \\frac{\\partial{\\text{MSE}}}{\\partial{w}}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to calculate the cost\n",
    "def compute_cost(x, y, w, b):\n",
    "    m = x.shape[0] \n",
    "    cost = 0\n",
    "    \n",
    "    for i in range(m):\n",
    "        f_wb = w * x[i] + b\n",
    "        cost = cost + (f_wb - y[i])**2\n",
    "    total_cost = 1 / (2 * m) * cost\n",
    "\n",
    "    return total_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(x, y, w, b): \n",
    "    \"\"\"\n",
    "    Computes the gradient for linear regression \n",
    "    Args:\n",
    "      x (ndarray (m,)): Data, m examples \n",
    "      y (ndarray (m,)): target values\n",
    "      w,b (scalar)    : model parameters  \n",
    "    Returns\n",
    "      dj_dw (scalar): The gradient of the cost w.r.t. the parameters w\n",
    "      dj_db (scalar): The gradient of the cost w.r.t. the parameter b     \n",
    "     \"\"\"\n",
    "    \n",
    "    # Number of training examples\n",
    "    m = x.shape[0]    \n",
    "    dj_dw = 0\n",
    "    dj_db = 0\n",
    "    \n",
    "    for i in range(m):  \n",
    "        f_wb = w * x[i] + b \n",
    "        dj_dw_i = (f_wb - y[i]) * x[i] \n",
    "        dj_db_i = f_wb - y[i] \n",
    "        dj_db += dj_db_i\n",
    "        dj_dw += dj_dw_i \n",
    "    dj_dw = dj_dw / m \n",
    "    dj_db = dj_db / m \n",
    "        \n",
    "    return dj_dw, dj_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, copy\n",
    "def gradient_descent(x, y, w_in, b_in, alpha, num_iters, cost_function, gradient_function): \n",
    "    \"\"\"\n",
    "    Performs gradient descent to fit w,b. Updates w,b by taking \n",
    "    num_iters gradient steps with learning rate alpha\n",
    "    \n",
    "    Args:\n",
    "      x (ndarray (m,))  : Data, m examples \n",
    "      y (ndarray (m,))  : target values\n",
    "      w_in,b_in (scalar): initial values of model parameters  \n",
    "      alpha (float):     Learning rate\n",
    "      num_iters (int):   number of iterations to run gradient descent\n",
    "      cost_function:     function to call to produce cost\n",
    "      gradient_function: function to call to produce gradient\n",
    "      \n",
    "    Returns:\n",
    "      w (scalar): Updated value of parameter after running gradient descent\n",
    "      b (scalar): Updated value of parameter after running gradient descent\n",
    "      J_history (List): History of cost values\n",
    "      p_history (list): History of parameters [w,b] \n",
    "      \"\"\"\n",
    "    \n",
    "    w = copy.deepcopy(w_in) # avoid modifying global w_in\n",
    "    # An array to store cost J and w's at each iteration primarily for graphing later\n",
    "    J_history = []\n",
    "    p_history = []\n",
    "    b = b_in\n",
    "    w = w_in\n",
    "    \n",
    "    for i in range(num_iters):\n",
    "        # Calculate the gradient and update the parameters using gradient_function\n",
    "        dj_dw, dj_db = gradient_function(x, y, w , b)     \n",
    "\n",
    "        # Update Parameters using equation (3) above\n",
    "        b = b - alpha * dj_db                            \n",
    "        w = w - alpha * dj_dw                            \n",
    "\n",
    "        # Save cost J at each iteration\n",
    "        if i<100000:      # prevent resource exhaustion \n",
    "            J_history.append( cost_function(x, y, w , b))\n",
    "            p_history.append([w,b])\n",
    "        # Print cost every at intervals 10 times or as many iterations if < 10\n",
    "        if i% math.ceil(num_iters/10) == 0:\n",
    "            # print(f\"Iteration {i:4}: Cost {J_history[-1]:0.2e} \",\n",
    "            #       f\"dj_dw: {dj_dw: 0.3e}, dj_db: {dj_db: 0.3e}  \",\n",
    "            #       f\"w: {w: 0.3e}, b:{b: 0.5e}\")\n",
    "            print(f\"Iteration {i:4}: Cost {J_history[-1]} \",\n",
    "                  f\"dj_dw: {dj_dw}, dj_db: {dj_db}  \",\n",
    "                  f\"w: {w}, b:{b}\")\n",
    " \n",
    "    return w, b, J_history, p_history #return w and J,w history for graphing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Iteration |  w    | b     |  Cost |  dJ/dw   |   dJ/db  |\n",
    "|---|---|---|---|---|---|\n",
    "|         0| 0.0110| 0.0195| 2.0443|   -1.1077|   -1.9538|\n",
    "|      1000| 1.4309| 1.2985| 0.0178|   -0.0407|   0.02078|\n",
    "|      2000| 1.7162| 1.1527| 0.0071|   -0.0191|   0.00976|\n",
    "|      3000| 1.8502| 1.0842| 0.0047|   -0.0090|   0.00459|\n",
    "|      4000| 1.9132| 1.0520| 0.0042|   -0.0042|   0.00215|\n",
    "|      5000| 1.9430| 1.0369| 0.0041|   -0.0020|   0.00101|\n",
    "|      6000| 1.9567| 1.0298| 0.0040|   -0.0009|   0.00047|\n",
    "|      7000| 1.9632| 1.0265| 0.0040|   -0.0004|   0.00022|\n",
    "|      8000| 1.9663| 1.0249| 0.0040|   -0.0002|   0.00010|\n",
    "|      9000| 1.9677| 1.0242| 0.0040|-9.637e-05| 4.925e-05|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize parameters\n",
    "w_init = 0\n",
    "b_init = 0\n",
    "# some gradient descent settings\n",
    "iterations = 10000\n",
    "tmp_alpha = 1.0e-2\n",
    "# run gradient descent\n",
    "w_final, b_final, J_hist, p_hist = gradient_descent(X_train ,y_train, w_init, b_init, tmp_alpha, \n",
    "                                                    iterations, compute_cost, compute_gradient)\n",
    "print(f\"(w,b) found by gradient descent: ({w_final},{b_final})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot cost versus iteration  \n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, constrained_layout=True, figsize=(12,4))\n",
    "ax1.plot(J_hist[:100])\n",
    "ax2.plot(1000 + np.arange(len(J_hist[1000:])), J_hist[1000:])\n",
    "ax1.set_title(\"Cost vs. iteration(start)\");  ax2.set_title(\"Cost vs. iteration (end)\")\n",
    "ax1.set_ylabel('Cost')            ;  ax2.set_ylabel('Cost') \n",
    "ax1.set_xlabel('iteration step')  ;  ax2.set_xlabel('iteration step') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = X_train.shape[0]\n",
    "predicted = np.zeros(m)\n",
    "\n",
    "for i in range(m):\n",
    "    predicted[i] = w_final * X_train[i] + b_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6.1 Inference with Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the linear fit\n",
    "plt.plot(X_train, predicted, c = \"b\")\n",
    "\n",
    "# Create a scatter plot of the data. \n",
    "plt.scatter(X_train, y_train, marker='x', c='r') \n",
    "\n",
    "# Set the title\n",
    "plt.title(\"Inference with Linear Regression\")\n",
    "# Set the y-axis label\n",
    "plt.ylabel('y')\n",
    "# Set the x-axis label\n",
    "plt.xlabel('x')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Impact of StandardScaler on Gradient Descent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_b = 1\n",
    "true_w = 2\n",
    "N = 100\n",
    "\n",
    "# Data Generation\n",
    "np.random.seed(42)\n",
    "\n",
    "# We divide w by 100\n",
    "bad_w = true_w / 100\n",
    "# And multiply x by 10\n",
    "bad_x = np.random.rand(N, 1) * 100\n",
    "\n",
    "# So, the net effect on y is zero - it is still\n",
    "# the same as before\n",
    "y = true_b + bad_w * bad_x + (.1 * np.random.randn(N, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates train and validation sets\n",
    "# It uses the same train_idx and val_idx as before,\n",
    "# but it applies to bad_x\n",
    "bad_x_train, y_train = bad_x[train_idx], y[train_idx]\n",
    "bad_x_val, y_val = bad_x[val_idx], y[val_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
    "ax[0].scatter(X_train, y_train)\n",
    "ax[0].set_xlabel('x')\n",
    "ax[0].set_ylabel('y')\n",
    "ax[0].set_ylim([0, 3.1])\n",
    "ax[0].set_title('Train - Original')\n",
    "ax[1].scatter(bad_x_train, y_train, c='k')\n",
    "ax[1].set_xlabel('x')\n",
    "ax[1].set_ylabel('y')\n",
    "ax[1].set_ylim([0, 3.1])\n",
    "ax[1].set_title('Train - \"Bad\"')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The ranges CHANGED because we are centering at the new minimum, using \"bad\" data\n",
    "bad_b_range = np.linspace(-2, 4, 101)\n",
    "bad_w_range = np.linspace(-2.8, 3.2, 101)\n",
    "bad_bs, bad_ws = np.meshgrid(bad_b_range, bad_w_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_surface_good_bad(X_train, y_train, b_initial, w_initial, bad_bs, bad_ws, bad_x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_function_good_bad(X_train, y_train, b_initial, w_initial, bad_bs, bad_ws, bad_x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler(with_mean=True, with_std=True)\n",
    "# We use the TRAIN set ONLY to fit the scaler\n",
    "scaler.fit(X_train)\n",
    "\n",
    "# Now we can use the already fit scaler to TRANSFORM\n",
    "# both TRAIN and VALIDATION sets\n",
    "scaled_x_train = scaler.transform(X_train)\n",
    "scaled_x_val = scaler.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The ranges CHANGED AGAIN because we are centering at the new minimum, using \"scaled\" data\n",
    "scaled_b_range = np.linspace(-1, 5, 101)\n",
    "scaled_w_range = np.linspace(-2.4, 3.6, 101)\n",
    "scaled_bs, scaled_ws = np.meshgrid(scaled_b_range, scaled_w_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_surface_good_bad_scaled(X_train, y_train, scaled_bs, scaled_ws, bad_x_train, scaled_x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-oct23",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
