{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d53ylTKoavaE",
      "metadata": {
        "id": "d53ylTKoavaE"
      },
      "source": [
        "Load essential libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11cd04ba",
      "metadata": {
        "id": "11cd04ba"
      },
      "outputs": [],
      "source": [
        "## Load essential libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sys\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.cm as cm\n",
        "plt.style.use('seaborn-whitegrid')\n",
        "%matplotlib inline\n",
        "\n",
        "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2OiQVoA4a5iX",
      "metadata": {
        "id": "2OiQVoA4a5iX"
      },
      "source": [
        "Mount the Google Drive folder, if needed, for accessing data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8N869o4CHSl3",
      "metadata": {
        "id": "8N869o4CHSl3"
      },
      "outputs": [],
      "source": [
        "# ## Mount the Google Drive folder, if needed, for accessing data\n",
        "# if('google.colab' in sys.modules):\n",
        "#     from google.colab import drive\n",
        "#     drive.mount('/content/drive', force_remount = True)\n",
        "#     # Change path below starting from /content/drive/MyDrive/Colab Notebooks/\n",
        "#     # depending on how data is organized inside your Colab Notebooks folder in\n",
        "#     # Google Drive\n",
        "#     DIR = '/content/drive/MyDrive/Colab Notebooks/MAHE/MSIS Coursework/OddSem2023MAHE'\n",
        "#     DATA_DIR = DIR+'/Data/'\n",
        "# else:\n",
        "#     DATA_DIR = 'Data/'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "JzzzBexZa73j",
      "metadata": {
        "id": "JzzzBexZa73j"
      },
      "source": [
        "Load the ICU dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "YOyUpg1xaTAz",
      "metadata": {
        "id": "YOyUpg1xaTAz"
      },
      "outputs": [],
      "source": [
        "## Load the ICU dataset\n",
        "#FILENAME = DATA_DIR + 'ICU_Complete.csv'\n",
        "FILENAME =  'Data/ICU_Complete.csv'\n",
        "df = pd.read_csv(FILENAME)\n",
        "print('ICU dataset')\n",
        "print('-----------')\n",
        "print(df.info(verbose = True))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "226ca4ea",
      "metadata": {},
      "source": [
        "in hospital death is target variable"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "W1oGyw1-fJhM",
      "metadata": {
        "id": "W1oGyw1-fJhM"
      },
      "source": [
        "Create lists of categorical and continuous features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3VlYBvafNxU",
      "metadata": {
        "id": "e3VlYBvafNxU"
      },
      "outputs": [],
      "source": [
        "## Create lists of categorical and continuous features\n",
        "categorical_features = [ 'In-hospital_death','Gender', 'MechVent']\n",
        "continuous_features = df.columns[~df.columns.isin(categorical_features)].to_list()\n",
        "\n",
        "\n",
        "target_variable= 'In-hospital_death'\n",
        "categorical_features.remove(target_variable)\n",
        "\n",
        "print(\"categorical_features\")\n",
        "print(categorical_features)\n",
        "print(\"continuous_features\")\n",
        "print(continuous_features)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "DzjCy2FngP1r",
      "metadata": {
        "id": "DzjCy2FngP1r"
      },
      "source": [
        "Assign 'category' datatype to categorical columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "291d9c84",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define the sigmoid function\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "# Create an array of x values from -10 to 10\n",
        "x = np.linspace(-10, 10, 400)\n",
        "\n",
        "# Calculate the corresponding y values using the sigmoid function\n",
        "y = sigmoid(x)\n",
        "\n",
        "# Create the plot\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(x, y, label='Sigmoid Function', color='b')\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('Sigmoid(x)')\n",
        "plt.title('Sigmoid Function')\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "o7nU62nGi-ek",
      "metadata": {
        "id": "o7nU62nGi-ek"
      },
      "source": [
        "One-hot encode the categorical features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hHcfJeF9i8-6",
      "metadata": {
        "id": "hHcfJeF9i8-6"
      },
      "outputs": [],
      "source": [
        "## One-hot encode the categorical features\n",
        "df = pd.concat([df, pd.get_dummies(df[categorical_features])], axis = 1).drop(categorical_features, axis = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "51367622",
      "metadata": {},
      "outputs": [],
      "source": [
        "# pd.get_dummies(df[categorical_features])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7b1wjqIojPdH",
      "metadata": {
        "id": "7b1wjqIojPdH"
      },
      "source": [
        "Label encode the target variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ElEWDNhAjYlf",
      "metadata": {
        "id": "ElEWDNhAjYlf"
      },
      "outputs": [],
      "source": [
        "## Label encode the output label\n",
        "labenc = LabelEncoder()\n",
        "df[target_variable] = labenc.fit_transform(df[target_variable])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "g2fvl6R0ixLZ",
      "metadata": {
        "id": "g2fvl6R0ixLZ"
      },
      "source": [
        "How balanced is the dataset w.r.t. the target variable?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5AUidZJbiveB",
      "metadata": {
        "id": "5AUidZJbiveB"
      },
      "outputs": [],
      "source": [
        "## How balanced is the dataset w.r.t. the\n",
        "## target variable 'In-hospital_death'?\n",
        "print(np.mean(df['In-hospital_death'] == 0)*100)\n",
        "print(np.mean(df['In-hospital_death'] == 1)*100)\n",
        "df['In-hospital_death'].value_counts().plot(kind = 'barh')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "VcKDeYxvlpmN",
      "metadata": {
        "id": "VcKDeYxvlpmN"
      },
      "source": [
        "The final dataframe for analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "o96V_YhQlrtv",
      "metadata": {
        "id": "o96V_YhQlrtv"
      },
      "outputs": [],
      "source": [
        "## The final dataframe for analysis\n",
        "df.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "D2QJ3VeVmW4e",
      "metadata": {
        "id": "D2QJ3VeVmW4e"
      },
      "source": [
        "Stratified train and test split of the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "G-j85ye1mUx_",
      "metadata": {
        "id": "G-j85ye1mUx_"
      },
      "outputs": [],
      "source": [
        "## Stratified train and test split of the data\n",
        "X = df.drop('In-hospital_death', axis = 1)\n",
        "y = df['In-hospital_death']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
        "                                                    stratify = y,\n",
        "                                                    test_size = 0.2,\n",
        "                                                    random_state = 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "rKiJc4cFmsjl",
      "metadata": {
        "id": "rKiJc4cFmsjl"
      },
      "source": [
        "Check the proportion of output labels in train and test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rDOkaEcxmnTm",
      "metadata": {
        "id": "rDOkaEcxmnTm"
      },
      "outputs": [],
      "source": [
        "## Check the proportion of output labels in train and test set\n",
        "print('Proportion of 1-to-0 labels in train set = %f, test set = %f\\n'%\n",
        "      (np.mean(y_train == 1), np.mean(y_test == 1)))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "hW0zPbuqnzZN",
      "metadata": {
        "id": "hW0zPbuqnzZN"
      },
      "source": [
        "Logistic regression starts from here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "tu93UDOHn5mn",
      "metadata": {
        "id": "tu93UDOHn5mn"
      },
      "outputs": [],
      "source": [
        "num_samples =  X.shape[0] # number of samples\n",
        "num_features = X.shape[1] # number of features (a.k.a. dimensionality)\n",
        "num_labels = len(np.unique(y)) # number of unique target variable labels"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "QgCccBbvo74t",
      "metadata": {
        "id": "QgCccBbvo74t"
      },
      "source": [
        "Add bias feature to all samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "a3a658a8",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "16"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "num_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "LLoYT1Neo8S1",
      "metadata": {
        "id": "LLoYT1Neo8S1"
      },
      "outputs": [],
      "source": [
        "## Add bias feature to all samples\n",
        "X = np.hstack([X, np.ones((X.shape[0], 1))])\n",
        "num_features += 1"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "xjG2wb28pN_j",
      "metadata": {
        "id": "xjG2wb28pN_j"
      },
      "source": [
        "Initial weight vector including bias term"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "pagJi3pOpBl_",
      "metadata": {
        "id": "pagJi3pOpBl_"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "w = \n",
            "[-0.1 -0.1 -0.1 -0.1 -0.1 -0.1 -0.1 -0.1 -0.1 -0.1 -0.1 -0.1 -0.1 -0.1\n",
            " -0.1 -0.1 -0.1]\n"
          ]
        }
      ],
      "source": [
        "## Initial weight vector including bias term\n",
        "\n",
        "w = np.random.choice(np.arange(-0.1, 0.1), size=num_features, replace = True)\n",
        "print('w = ')\n",
        "print(w)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "UFzjeHdtpWrR",
      "metadata": {
        "id": "UFzjeHdtpWrR"
      },
      "source": [
        "Define sigmoid function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "lSdumesIpRqp",
      "metadata": {
        "id": "lSdumesIpRqp"
      },
      "outputs": [],
      "source": [
        "## Define sigmoid function\n",
        "sigmoid = lambda x : 1 / (1+np.exp(-x))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "YgluOAeJr0ge",
      "metadata": {
        "id": "YgluOAeJr0ge"
      },
      "source": [
        "Calculate raw and sigmoid-activated scores for all samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "JFavzM8Wr01w",
      "metadata": {
        "id": "JFavzM8Wr01w"
      },
      "outputs": [],
      "source": [
        "## Calculate raw scores for all samples\n",
        "z = np.dot(X, w)\n",
        "\n",
        "## Calculate sigmoid-activated scores for all samples\n",
        "a = sigmoid(z)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "LzJtV9ZTsBdZ",
      "metadata": {
        "id": "LzJtV9ZTsBdZ"
      },
      "source": [
        "Calculate average data loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "iQceqIWPr7Wl",
      "metadata": {
        "id": "iQceqIWPr7Wl"
      },
      "outputs": [],
      "source": [
        "## Calculate total average data loss\n",
        "loss_data = np.mean(y*np.log(a) + (1-y)*np.log(1-a))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Uq0KumXlsMjw",
      "metadata": {
        "id": "Uq0KumXlsMjw"
      },
      "source": [
        "Calculate regularization loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "pQilo0LgsCd5",
      "metadata": {
        "id": "pQilo0LgsCd5"
      },
      "outputs": [],
      "source": [
        "## Calculate regularization loss\n",
        "reg = 0.1 # regularization strength\n",
        "loss_reg = reg * np.sum(w * w)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bhIGmnjjsacL",
      "metadata": {
        "id": "bhIGmnjjsacL"
      },
      "source": [
        "Calculate total loss as sum of data loss and regularization loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "5E46GfWMsQc8",
      "metadata": {
        "id": "5E46GfWMsQc8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total loss = data loss + regularization loss\n",
            "-15.076904277848369\n"
          ]
        }
      ],
      "source": [
        "## Calculate total loss as sum of data loss and regularization loss\n",
        "loss = loss_data + loss_reg\n",
        "print('Total loss = data loss + regularization loss')\n",
        "print(loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "BYhezgWJsrA_",
      "metadata": {
        "id": "BYhezgWJsrA_"
      },
      "source": [
        "Calculate the gradient w.r.t. weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "KartTZFRsgyR",
      "metadata": {
        "id": "KartTZFRsgyR"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Gradient vector = \n",
            "[ -2.06741744  -9.22321761 -22.50060965  -8.02550381 -21.52508044\n",
            " -12.53058425  -4.87770533  -5.47215919 -23.79984759 -27.34387807\n",
            "  -1.9795597  -11.36949196  -0.07884843  -0.09451312  -0.03312447\n",
            "  -0.14023709  -0.13336156]\n"
          ]
        }
      ],
      "source": [
        "## Calculate the gradient w.r.t. weights\n",
        "## Sum of gradient of average data loss and the gradient of the regularization loss\n",
        "dw = (1 / num_samples) * np.dot(X.T, (a - y)) + 2 * reg * np.hstack([w[:-1], 0])\n",
        "print('Gradient vector = ')\n",
        "print(dw)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.17"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
