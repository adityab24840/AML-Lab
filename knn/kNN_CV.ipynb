{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# kNN\n",
    "\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "**kNN with scikit-learn**\n",
    "1. [kNN based plain vanilla ML steps](#knn-vanilla)\n",
    "2. [Cross validation for k using scikit learn API](#cv)\n",
    "3. [GridSearch & RandomSearch for multiple hyperparameter tuning](#gridsearch)\n",
    "4. [Beyond GridSearch - Lack of stability even after GridSearchCV (Optional)](#beyond-gridsearch)\n",
    "5. [Cross validation from scratch](cv-from-scratch)\n",
    "\n",
    "**Assignments**\n",
    "1. [train test split from scratch](#train-test-scratch)\n",
    "2. [kNN from scratch](#knn-scratch)\n",
    "3. [GridSearch from scratch](#gridsearch-scratch)\n",
    "4. [Integrate your custom code](#integrate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import sklearn as sk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"numpy version = {np.__version__}\")  #Check the numpy version for sanity check\n",
    "print(f\"sklearn version = {sk.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='knn-vanilla'></a>\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## kNN with scikit-learn\n",
    "\n",
    "### 1. kNN based plain vanilla ML steps\n",
    "\n",
    "##### 1.1 Acquire and load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import load_iris function from datasets module\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# save \"bunch\" object containing iris dataset and its attributes\n",
    "iris = load_iris()\n",
    "\n",
    "#store feature matrix in dataframe\n",
    "df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "\n",
    "# store response vector in \"y\"\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the shapes of X and y\n",
    "print(df.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.2 3 step ML process\n",
    "\n",
    "1. Model Selection\n",
    "2. Model Training (plus Tuning)\n",
    "3. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start by train/test split data with random_state \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df, y, random_state=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"type(X_train)={type(X_train)}\") # when dataframe is passed, the split is also dataframe\n",
    "print(f\"type(y_train)={type(y_train)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Model Selection\n",
    "# import the algorithm instantiate the algorithm class\n",
    "from sklearn.neighbors import KNeighborsClassifier \n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Model Training (& Tuning)\n",
    "knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Model Evaluation (Prediction)\n",
    "\n",
    "from sklearn import metrics\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy with k = 1 {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='cv'></a>\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Cross validation for k using scikit learn API\n",
    "\n",
    "K-Fold CV diagram \n",
    "\n",
    "![CV](https://onedrive.live.com/embed?resid=A5A4158EF1352FCB%211929&authkey=%21AOCi-KlSCxdsSqo&width=631&height=279)\n",
    "\n",
    "\n",
    "**Different types of Validation**\n",
    "\n",
    "1. Holdout: In some sense, Train-Test Split is also one extreme of K-Fold Cross validation. Here we can assume the entire X is split into 2 parts (k=2). The train split is used for model training; test part is used for model testing. Test split is called Holdout. And this validation method is also referred to as Hold-out Cross Validation\n",
    "\n",
    "2. K-Fold CV: The K-Fold Cross Validation we have adopted is a practical combination of holdout and the pure form of K-Fold Cross validation. We first do the split for hold out and then execute K-Fold CV as if it is complete dataset. If data is scarce, then we could first choose 90:10 train-test split instead of 80:20, followed a higher K for K-Fold CV.\n",
    "\n",
    "3. Stratified CV: When there is some class imbalance in the dataset, we would like to get each class a representative share in model training and validation, which otherwise may not be possible with random shuffle and sampling. Rest of this is same as K-Fold CV\n",
    "\n",
    "4. Leave-One-Out CV (LOOCV): is another extreme from holdout, where the total number of folds = number of observations (in X_train as applied to our case). If X_train split contained n rows, the model is trained on n-1 and validated on 1. This is computationally very expensive. But this should be used if the dataset is quite small.\n",
    "\n",
    "5. Repeated K-Fold CV: K-Fold CV repeated and/or stratified\n",
    "6. Nested CV: This is nothing but GridSearchCV and RandomSearch CV\n",
    "\n",
    "Question: What is the computational cost of each type of CV? \n",
    "\n",
    "\n",
    "##### 2.1 K-Fold CV\n",
    "\n",
    "1. K-Fold CV divides dataset into k non-overlapping folds. \n",
    "2. Each of the k folds gets an opportunity to be used as hold-out test set, when all other folds collectively are used as training dataset. \n",
    "3. A total of k models are fit and evaluated on the k hold-out test sets and the mean performance is reported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=2)\n",
    "scores = cross_val_score(knn, X_train, y_train, cv=5, scoring='accuracy')\n",
    "\n",
    "print(f\"Cross Validation scores = {scores}\")\n",
    "print(f\"type={type(scores)}\")\n",
    "print(f\"Average CV accuracy for k=2 is {np.mean(scores)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search for an optimal value of K for KNN\n",
    "k_range = list(range(1, 20))\n",
    "k_avg_scores = []\n",
    "for k in k_range:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    scores = cross_val_score(knn, X_train, y_train, cv=5, scoring='accuracy')\n",
    "    k_avg_scores.append(scores.mean())\n",
    "print(np.round(np.array(k_avg_scores), decimals=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# plot the value of K for KNN (x-axis) versus the cross-validated accuracy (y-axis)\n",
    "plt.plot(k_range, k_avg_scores)\n",
    "plt.xlabel('k for KNN')\n",
    "plt.ylabel('CV Accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose best k\n",
    "np.argmax(np.array(k_avg_scores))+1 #why the +1?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Whats different from previous code cell? \n",
    "# KFold object is explicitly provided for cv instead of mere number.\n",
    "# This allows fine grained control over the KFold\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df, y, random_state=4)\n",
    "\n",
    "k_range = list(range(1, 25))\n",
    "k_avg_scores = []\n",
    "\n",
    "for k in k_range:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    kfold = KFold(n_splits=5, shuffle=False) #this is new in this code cell\n",
    "    scores = cross_val_score(knn, X_train, y_train, cv=kfold, scoring='accuracy') \n",
    "    k_avg_scores.append(scores.mean())\n",
    "print(np.round(np.array(k_avg_scores), decimals=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.2 Leave One Out Cross Validation (LOOCV)\n",
    "\n",
    "1. This is equivalent to saying n-fold CV. where n is the number of records. Each record is a fold\n",
    "2. n-1 folds are combined to form the training portion. 1 record is used for validation\n",
    "3. Very effective for checking hyperparameters, especially when data is very less, but computationally very expensive "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import LeaveOneOut\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df, y, random_state=4)\n",
    "\n",
    "k_range = list(range(1, 25))\n",
    "k_avg_scores = []\n",
    "for k in k_range:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    loocv = LeaveOneOut()\n",
    "    scores = cross_val_score(knn, X_train, y_train, cv=loocv, scoring='accuracy') #Leave One out CV object provided for cv\n",
    "    k_avg_scores.append(scores.mean())\n",
    "\n",
    "print(np.round(np.array(k_avg_scores), decimals=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_k = np.argmax(np.array(k_avg_scores))+1\n",
    "print(f\"Best value for k is {best_k}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the value of K for KNN (x-axis) versus the cross-validated accuracy (y-axis)\n",
    "plt.plot(k_range, k_avg_scores)\n",
    "plt.xlabel('k for KNN')\n",
    "plt.ylabel('CV Accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.3 Stratified K-Fold CV\n",
    "\n",
    "1. Best option for CV on imbalanced dataset\n",
    "2. For a generic parametric model like Logistic Regression etc., the impact of higher class imbalance on hyperparameter tuning with Stratified K-Fold is that it will get a representative train and test dataset and discover higher & better L2 Regularization lambda\n",
    "3. For kNN, however, the model reacts by finding a lower k to increase its accuracy. This is undesirable side effect of kNN on imbalanced dataset. We attempt to fix this by setting the lower k range to be at least 5\n",
    "4. kNN is a very bad choice for imbalanced dataset\n",
    "5. Also accuracy is not the best measure for model performance in imbalanced dataset\n",
    "6. In general the model trained with stratified split and stratified kfold is robust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stratified CV example with a synthetic dataset\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# generate data\n",
    "X_imb, y_imb = make_classification(n_samples=1000, n_classes=2, weights=[0.95, 0.05], flip_y=0, random_state=1)\n",
    "\n",
    "X_imb_train, X_imb_test, y_imb_train, y_imb_test = \\\n",
    "    train_test_split(X_imb, y_imb, test_size = 0.2, stratify=y_imb, shuffle = True, random_state = 0)\n",
    "\n",
    "X_imb_train[0:5,0:2], y_imb_train[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize dataset\n",
    "classes = np.unique(y_imb)\n",
    "total = len(y_imb)\n",
    "for c in classes:\n",
    " n_examples = len(y_imb[y_imb==c])\n",
    " percent = n_examples / total * 100\n",
    " print('> Class=%d : %d/%d (%.1f%%)' % (c, n_examples, total, percent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_lower_limit = 3\n",
    "k_range = list(range(k_lower_limit, 25))\n",
    "k_avg_scores = []\n",
    "for k in k_range:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    kfold = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "    scores = cross_val_score(knn, X_imb_train, y_imb_train, cv=kfold, scoring='accuracy')\n",
    "    k_avg_scores.append(scores.mean())\n",
    "\n",
    "print(np.round(np.array(k_avg_scores), decimals=3))\n",
    "\n",
    "best_k = np.argmax(np.array(k_avg_scores)) + k_lower_limit\n",
    "print(f\"Best value for k is {best_k}\")\n",
    "\n",
    "# Fit the model again with best k obtained in cross validation\n",
    "knn1 = KNeighborsClassifier(n_neighbors=best_k) \n",
    "knn1.fit(X_imb_train, y_imb_train)\n",
    "y_imb_pred = knn1.predict(X_imb_test)\n",
    "accuracy = metrics.accuracy_score(y_imb_test, y_imb_pred)\n",
    "print(f\"Test accuracy with K-Fold and best k = {best_k} is {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_lower_limit = 3\n",
    "k_range = list(range(k_lower_limit, 25))\n",
    "k_avg_scores = []\n",
    "for k in k_range:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    skfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n",
    "    scores = cross_val_score(knn, X_imb_train, y_imb_train, cv=skfold, scoring='accuracy')\n",
    "    k_avg_scores.append(scores.mean())\n",
    "\n",
    "print(np.round(np.array(k_avg_scores), decimals=3))\n",
    "\n",
    "best_k = np.argmax(np.array(k_avg_scores)) + k_lower_limit\n",
    "print(f\"Best value for k is {best_k}\")\n",
    "\n",
    "# Fit the model again with best k obtained in cross validation\n",
    "knn2 = KNeighborsClassifier(n_neighbors=best_k) \n",
    "knn2.fit(X_imb_train, y_imb_train)\n",
    "y_imb_pred = knn2.predict(X_imb_test)\n",
    "accuracy = metrics.accuracy_score(y_imb_test, y_imb_pred)\n",
    "print(f\"Accuracy with Stratified K-Fold and best k = {best_k} is {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.4 Repeated K-Fold CV\n",
    "\n",
    "1. K-Fold runs on different train and val folds in each iteration. This provides a noisy estimate (by design)\n",
    "2. One way to reduce the noise is increase k in K-Fold. This reduces the bias in estimated model performance, but increases variance\n",
    "3. One solution is to repeat each fold of the K-Fold a certain number of times and report the mean metrics of the repeats for that given KFold. This fixes the variance issue  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df, y, random_state=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn1 = KNeighborsClassifier(n_neighbors=3)\n",
    "kfold1 = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "scores1 = cross_val_score(knn1, X_train, y_train, cv=kfold1, scoring='accuracy')\n",
    "\n",
    "knn2 = KNeighborsClassifier(n_neighbors=3)\n",
    "kfold2 = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "scores2 = cross_val_score(knn2, X_train, y_train, cv=kfold2, scoring='accuracy')\n",
    "\n",
    "print(f\"Variance of scores in 5-Fold = {np.var(scores1)}\")\n",
    "print(f\"Variance of scores in 10-Fold = {np.var(scores2)}\")\n",
    "scores1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RepeatedKFold\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "rkfold = RepeatedKFold(n_splits=5, n_repeats=2, random_state=42)\n",
    "scores3 = cross_val_score(knn, X_train, y_train, cv=rkfold, scoring='accuracy')\n",
    "\n",
    "print(f\"Variance of scores in Repeated 5-Fold = {np.var(scores3)}\")\n",
    "scores3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(1,3,1)\n",
    "plt.boxplot(scores1)\n",
    "plt.subplot(1,3,2)\n",
    "plt.boxplot(scores2)\n",
    "plt.subplot(1,3,3)\n",
    "plt.boxplot(scores3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='gridsearch'></a>\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 GridSearch & RandomSearch for multiple hyperparameter tuning\n",
    "\n",
    "##### 3.1 Grid Search\n",
    "1. The code below uses LOOCV for CV. We could even use K-Fold CV instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df, y, random_state=42)\n",
    "\n",
    "#create new a knn model\n",
    "knn = KNeighborsClassifier()\n",
    "#create a dictionary of all values we want to test for n_neighbors\n",
    "param_grid = {\"n_neighbors\": np.arange(1, 25), \"weights\": [\"uniform\", \"distance\"]}\n",
    "\n",
    "loocv = LeaveOneOut()\n",
    "\n",
    "#use gridsearch to test all values for n_neighbors\n",
    "knn_gscv = GridSearchCV(knn, param_grid, cv=loocv) #can also specify something like cv=5 for a plain KFold \n",
    "#fit model to data\n",
    "knn_gscv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check top performing n_neighbors value\n",
    "knn_gscv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(knn_gscv.cv_results_))\n",
    "print(knn_gscv.cv_results_.keys())\n",
    "print(np.array(knn_gscv.cv_results_[\"mean_test_score\"]))\n",
    "print(np.array(knn_gscv.cv_results_[\"mean_test_score\"]).shape)\n",
    "print(np.argmax(np.array(knn_gscv.cv_results_[\"mean_test_score\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.2 Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df, y, random_state=42)\n",
    "\n",
    "#create new a knn model\n",
    "knn = KNeighborsClassifier()\n",
    "#create a dictionary of all values we want to test for n_neighbors\n",
    "param_grid = {\"n_neighbors\": np.arange(1, 25), \"weights\": [\"uniform\", \"distance\"]}\n",
    "\n",
    "loocv = LeaveOneOut()\n",
    "\n",
    "#use gridsearch to test all values for n_neighbors\n",
    "knn_rscv = RandomizedSearchCV(knn, param_grid, cv=loocv) #can also specify something like cv=5 for a plain KFold \n",
    "#fit model to data\n",
    "knn_rscv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check top performing n_neighbors value\n",
    "knn_rscv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(knn_rscv.cv_results_))\n",
    "print(knn_rscv.cv_results_.keys())\n",
    "print(np.array(knn_rscv.cv_results_[\"mean_test_score\"]))\n",
    "print(np.array(knn_rscv.cv_results_[\"mean_test_score\"]).shape)\n",
    "print(np.argmax(np.array(knn_rscv.cv_results_[\"mean_test_score\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='beyond-gridsearch'></a>\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 Beyond GridSearch - Lack of stability even after GridSearchCV (Optional)\n",
    "\n",
    "1. We run the KFold CV with two different data split and demonstrate that the k obtained even after Cross validation is not sufficient.\n",
    "2. Then we switch to LOOCV with different distance metric and repeat the process along with different train test split. This would have been generally achieved by a GridSearchCV in general.\n",
    "3. With the above two experiments we can conclude that even LOOCV based Grid Search is not going to be robust in selecting a good k value.\n",
    "4. Then we adopt a totally different approach of different train test split and running LOOCV and take a set intesection on the last 5 best scores. These methods from less charted path are generally used in Kaggle competitions  \n",
    "\n",
    "##### 4.1 KFold CV with one variety of train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We start by making a function out of KFold CV for reuse\n",
    "def knn_kfold_cv(X_train, y_train, mink=1, maxk=25):\n",
    "    k_range = list(range(mink, maxk))\n",
    "    k_avg_scores = []\n",
    "    for k in k_range:\n",
    "        knn = KNeighborsClassifier(n_neighbors=k)\n",
    "        scores = cross_val_score(knn, X_train, y_train, cv=5, scoring='accuracy')\n",
    "        k_avg_scores.append(scores.mean())\n",
    "\n",
    "    return k_avg_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_range = list(range(1, 25))\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, sharex=True, figsize=(10, 6))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df, y, random_state=4)\n",
    "k_avg_scores1 = knn_kfold_cv(X_train, y_train, mink=1, maxk=25)\n",
    "\n",
    "axes[0].plot(k_range, k_avg_scores1) #plt.plot(k_range, k_avg_scores)\n",
    "axes[0].set_xlabel('k for KNN') #plt.xlabel('k for KNN')\n",
    "axes[0].set_ylabel('CV Accuracy') #plt.ylabel('CV Accuracy')\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df, y, random_state=42)\n",
    "k_avg_scores2 = knn_kfold_cv(X_train, y_train, mink=1, maxk=25)\n",
    "\n",
    "axes[1].plot(k_range, k_avg_scores2)\n",
    "axes[1].set_xlabel('k for KNN')\n",
    "axes[1].set_ylabel('CV Accuracy')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.2 LOOCV with different train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For this purpose, we start by making a function out of CV\n",
    "def knn_loocv(X_train, y_train, mink=1, maxk=25):\n",
    "    k_range = list(range(mink, maxk))\n",
    "    k_avg_scores = []\n",
    "    for k in k_range:\n",
    "        knn = KNeighborsClassifier(n_neighbors=k, weights=\"distance\")\n",
    "        loocv = LeaveOneOut()\n",
    "        scores = cross_val_score(knn, X_train, y_train, cv=loocv, scoring='accuracy')\n",
    "        k_avg_scores.append(scores.mean())\n",
    "\n",
    "    return k_avg_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_range = list(range(1, 25))\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, sharex=True, figsize=(10, 6))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df, y, random_state=4)\n",
    "k_avg_scores1 = knn_loocv(X_train, y_train, mink=1, maxk=25)\n",
    "\n",
    "axes[0].plot(k_range, k_avg_scores1) #plt.plot(k_range, k_avg_scores)\n",
    "axes[0].set_xlabel('k for KNN') #plt.xlabel('k for KNN')\n",
    "axes[0].set_ylabel('CV Accuracy') #plt.ylabel('CV Accuracy')\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df, y, random_state=42)\n",
    "k_avg_scores2 = knn_loocv(X_train, y_train, mink=1, maxk=25)\n",
    "\n",
    "axes[1].plot(k_range, k_avg_scores2)\n",
    "axes[1].set_xlabel('k for KNN')\n",
    "axes[1].set_ylabel('CV Accuracy')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Intersection over the top5 score of both cross validation over different splits \n",
    "2. This is follwoeds by set intersection over the top 5 scores gives best and robust solution\n",
    "\n",
    "This comes atr a big cost. Lot more computing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max5_set1 = set(np.argsort(np.array(k_avg_scores1))[-5:])\n",
    "max5_set2 = set(np.argsort(np.array(k_avg_scores2))[-5:])\n",
    "\n",
    "max5_set1.intersection(max5_set2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='cv-from-scratch'></a>\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Cross validation from scratch (not exactly scratch scratch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrating data split in K-Fold Cross validation\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=False)\n",
    "\n",
    "#dummy data\n",
    "data = range(25)\n",
    "\n",
    "# print the contents of training and testing set\n",
    "print('{} {:^61} {}'.format('Iteration', 'Training set indices', 'Validation set indices'))\n",
    "iter_idx = 1\n",
    "for train_idx, val_idx in kf.split(data):\n",
    "    print('{:^9} {} {:^25}'.format(iter_idx, train_idx, str(val_idx)))\n",
    "    iter_idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross validation from scratch with k=2\n",
    "\n",
    "k_knn = 2 #How many k to use in kNN\n",
    "\n",
    "kfold_splits = 5 # how many splits in K-Fold CV\n",
    "\n",
    "kf = KFold(n_splits=kfold_splits, shuffle=False) #create the KFold object\n",
    "\n",
    "iter_idx = 0 # This looping cannot be obtained by enumerate. Hence manually defined\n",
    "\n",
    "accuracy_arr = np.empty(kfold_splits) #To store the accuracy of each iter in CV\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df, y, random_state=4)\n",
    "\n",
    "# kf.split returns indices on X_train \n",
    "for train_idx, val_idx in kf.split(X_train):\n",
    "\n",
    "    # split the original X_train into train and val data for this iter \n",
    "    X_train_cv = X_train.iloc[train_idx]\n",
    "    y_train_cv = y_train[train_idx]\n",
    "\n",
    "    X_val_cv =  X_train.iloc[val_idx]\n",
    "    y_val_cv = y_train[val_idx]\n",
    "\n",
    "    knn = KNeighborsClassifier(n_neighbors=k_knn)\n",
    "    knn.fit(X_train_cv, y_train_cv)\n",
    "    y_pred_cv = knn.predict(X_val_cv)\n",
    "    accuracy = metrics.accuracy_score(y_val_cv, y_pred_cv)\n",
    "   \n",
    "    print(f\"Iteration {iter_idx} score: {accuracy}\")\n",
    "\n",
    "    #store the accuracy of this iter in arr to calc mean accuracy later\n",
    "    accuracy_arr[iter_idx] = accuracy\n",
    "    iter_idx += 1\n",
    "\n",
    "print(f\"Average CV accuracy for k={k_knn} is {np.mean(accuracy_arr)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='train-test-scratch'></a>\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignments\n",
    "\n",
    "### 1. train test split from scratch\n",
    "\n",
    "Create a function my_train_test_split() that takes ipnput X, y and fraction of train. And ouputs the list or tuple containing splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clue 1: Splitting the data sequentially for a given fraction\n",
    "\n",
    "data = np.array([[1, 2, 0], [3, 4, 1], [5, 6, 1], [7, 8, 0], [9, 10, 1], [11, 12, 0]])\n",
    "print('data:')\n",
    "print(data)\n",
    "\n",
    "# Train part of the data\n",
    "split = int(0.8*data.shape[0])\n",
    "X_train = data[:split, :-1]\n",
    "y_train = data[:split, -1]\n",
    "\n",
    "print('\\nX_train:')\n",
    "print(X_train)\n",
    "print('\\ny_train:')\n",
    "print(y_train)\n",
    "\n",
    "# The test part of the data\n",
    "X_test = data[split:, :-1]\n",
    "y_test = data[split:, -1]\n",
    "\n",
    "print('\\ny_train:')\n",
    "print(X_test)\n",
    "\n",
    "print('\\ny_test:')\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clue2: splitting data randomly\n",
    "data = np.array([[1, 2, 0], [3, 4, 1], [5, 6, 1], [7, 8, 0]])\n",
    "num_samples = data.shape[0]\n",
    "ind = np.random.choice(num_samples, num_samples, replace = False)\n",
    "print(ind)\n",
    "print(type(ind))\n",
    "split = int(0.8*num_samples)\n",
    "print(split)\n",
    "ind[:split]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: Your function definition goes here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: Your function invocation goes here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='knn-scratch'></a>\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. kNN from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN class that allows setting the number of neighbours and weight=uniform or distance\n",
    "class KNN:\n",
    "    def __init__(self, ......): #Fill this out\n",
    "        pass\n",
    "\n",
    "    def fit(X, y): # What is missing in function definition?\n",
    "        pass\n",
    "\n",
    "    def predict(X): # What is missing in function definition?\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNN()\n",
    "knn.fit(??)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = knn.predict(?)\n",
    "accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy = {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='gridsearch-scratch'></a>\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. GridSearch from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clue: Look at itertools.product() functionality in python.\n",
    "# It will allow you to create Cartesian products needed for multiple hyperparam tuning\n",
    "# Use it in a loop to write your custom Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='integrate'></a>\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Integrate your custom code\n",
    "\n",
    "1. Create a dataframe of iris dataset\n",
    "2. Use your custom train test split function to split into train and test\n",
    "3. Use your custom GridSearch on your customKNN class to identify the best k and best weight for iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quickstart",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
